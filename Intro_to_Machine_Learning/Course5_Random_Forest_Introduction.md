# Course 5: Random Forests (Introduction & Example)

## 1. Introduction: ì™œ Random Forestì¸ê°€?

Decision TreeëŠ” êµ¬ì¡°ê°€ ë‹¨ìˆœí•˜ê³  í•´ì„ì´ ì‰½ì§€ë§Œ, **ê³¼ì†Œì í•©(Underfitting)** ê³¼ **ê³¼ì í•©(Overfitting)** ì‚¬ì´ì—ì„œ í•­ìƒ íŠ¸ë ˆì´ë“œì˜¤í”„ ë¬¸ì œê°€ ë°œìƒí•œë‹¤.

- ğŸŒ³ **ê¹Šì€ íŠ¸ë¦¬ (leafê°€ ë§ìŒ)**  
  â†’ ê° leafì— í¬í•¨ëœ ë°ì´í„°ê°€ ë§¤ìš° ì ìŒ  
  â†’ í›ˆë ¨ ë°ì´í„°ì—ëŠ” ì˜ ë§ì§€ë§Œ, ìƒˆë¡œìš´ ë°ì´í„°ì—ëŠ” ì„±ëŠ¥ ì €í•˜  
  â†’ **ê³¼ì í•©**

- ğŸŒ± **ì–•ì€ íŠ¸ë¦¬ (leafê°€ ì ìŒ)**  
  â†’ ë°ì´í„° êµ¬ë¶„ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŒ  
  â†’ í›ˆë ¨ ë°ì´í„°ì—ì„œë„ ì˜¤ì°¨ í¼  
  â†’ **ê³¼ì†Œì í•©**

ì´ëŸ¬í•œ ë¬¸ì œëŠ” Decision Treeë¿ ì•„ë‹ˆë¼, **ëŒ€ë¶€ë¶„ì˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ê³µí†µì ìœ¼ë¡œ ê²ªëŠ” ë¬¸ì œ**ë‹¤.

---

## 2. Random Forestì˜ í•µì‹¬ ì•„ì´ë””ì–´

**Random Forest**ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ëŒ€í‘œì ì¸ ì•™ìƒë¸”(Ensemble) ê¸°ë²•ì´ë‹¤.

### ê¸°ë³¸ ê°œë…
- ì—¬ëŸ¬ ê°œì˜ Decision Treeë¥¼ ë™ì‹œì— í•™ìŠµ
- ê° íŠ¸ë¦¬ëŠ” ì„œë¡œ **ì¡°ê¸ˆì”© ë‹¤ë¥¸ ë°ì´í„°/feature**ë¥¼ ì‚¬ìš©
- ìµœì¢… ì˜ˆì¸¡ì€ **ê° íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ê°’ì„ í‰ê· **í•˜ì—¬ ê³„ì‚°

ğŸ“Œ í•µì‹¬ íš¨ê³¼  
> ê°œë³„ íŠ¸ë¦¬ì˜ ê³¼ì í•© ê²½í–¥ì„ ì„œë¡œ ìƒì‡„í•˜ì—¬,  
> **ì¼ë°˜í™” ì„±ëŠ¥ì´ ë›°ì–´ë‚œ ëª¨ë¸**ì„ ë§Œë“ ë‹¤.

---

## 3. ë°ì´í„° ì¤€ë¹„ (ë³µìŠµ)

ì•„ë˜ ê³¼ì •ì€ ì´ì „ ì½”ìŠ¤ì—ì„œ ë°˜ë³µì ìœ¼ë¡œ ì‚¬ìš©í•œ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ê³¼ì •ì´ë‹¤.

```python
import pandas as pd

# Load data
melbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path)

# Filter rows with missing values
melbourne_data = melbourne_data.dropna(axis=0)

# Choose target and features
y = melbourne_data.Price
melbourne_features = [
    'Rooms', 'Bathroom', 'Landsize', 'BuildingArea',
    'YearBuilt', 'Lattitude', 'Longtitude'
]
X = melbourne_data[melbourne_features]
```

---

## 4. Train / Validation ë°ì´í„° ë¶„ë¦¬

ëª¨ë¸ ì„±ëŠ¥ì„ ê³µì •í•˜ê²Œ í‰ê°€í•˜ê¸° ìœ„í•´,  
ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„ë¦¬í•œë‹¤.

```python
from sklearn.model_selection import train_test_split

train_X, val_X, train_y, val_y = train_test_split(
    X, y, random_state=0
)
```

- `random_state=0`  
  â†’ í•­ìƒ ë™ì¼í•œ ë°ì´í„° ë¶„í•  ë³´ì¥ (ì¬í˜„ì„±)

---

## 5. Random Forest ëª¨ë¸ í•™ìŠµ

Decision Tree ëŒ€ì‹   
`RandomForestRegressor` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•œë‹¤.

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

forest_model = RandomForestRegressor(random_state=1)
forest_model.fit(train_X, train_y)

melb_preds = forest_model.predict(val_X)
print(mean_absolute_error(val_y, melb_preds))
```

### ì¶œë ¥ ê²°ê³¼
```
191669.7536453626
```

---

## 6. ê²°ê³¼ í•´ì„

- ğŸ“‰ **MAE â‰ˆ 191,670**
- ì´ì „ì— ìµœì í™”í•œ Decision Treeì˜ MAE â‰ˆ **250,000**
- **Random Forestê°€ ì•½ 6ë§Œ ì´ìƒ ì˜¤ì°¨ ê°ì†Œ**

ğŸ“Œ ì¤‘ìš”í•œ ì 
- ë³„ë„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì—†ì´ë„
- ë‹¨ì¼ Decision Treeë³´ë‹¤ **í›¨ì”¬ ì•ˆì •ì ì´ê³  ì •í™•**

---

## 7. Random Forestì˜ ì¥ì  ìš”ì•½

- âœ… ê³¼ì í•©ì— ê°•í•¨
- âœ… ê¸°ë³¸ ì„¤ì •ë§Œìœ¼ë¡œë„ ì„±ëŠ¥ì´ ì¢‹ìŒ
- âœ… Decision Treeì˜ ë‹¨ì ì„ íš¨ê³¼ì ìœ¼ë¡œ ë³´ì™„
- âŒ ëª¨ë¸ êµ¬ì¡° í•´ì„ì€ ìƒëŒ€ì ìœ¼ë¡œ ì–´ë ¤ì›€

---

## 8. ê²°ë¡ 

- ê³¼ì†Œì í•© / ê³¼ì í•© ë¬¸ì œëŠ” ëª¨ë“  ëª¨ë¸ì´ ì§ë©´
- Random ForestëŠ” **ì—¬ëŸ¬ íŠ¸ë¦¬ì˜ í‰ê· í™”**ë¡œ ì´ë¥¼ ì™„í™”
- ì‹¤ë¬´ ë° Kaggle ì…ë¬¸ ë‹¨ê³„ì—ì„œ **ê°€ì¥ ê°•ë ¥í•œ ê¸°ë³¸ ëª¨ë¸ ì¤‘ í•˜ë‚˜**
- ì¶”ê°€ì ì¸ ì„±ëŠ¥ í–¥ìƒì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ê°€ëŠ¥

---

## ë‹¤ìŒ ë‹¨ê³„ (Your Turn)

- `n_estimators`, `max_depth`, `max_leaf_nodes` ì¡°ì •
- Gradient Boosting, XGBoost ë“± ë‹¤ë¥¸ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ
- Feature Engineeringì„ í†µí•œ ì„±ëŠ¥ ê°œì„ 
