# Course 4: Underfitting and Overfitting (Model Tuning)

## í•™ìŠµ ëª©í‘œ
ì´ ë‹¨ê³„ì˜ ëª©í‘œëŠ” **ê³¼ì†Œì í•©(Underfitting)** ê³¼ **ê³¼ì í•©(Overfitting)** ì˜ ê°œë…ì„ ì´í•´í•˜ê³ ,  
ì´ë¥¼ í™œìš©í•´ **ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ë” ì •í™•í•œ ëª¨ë¸**ì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì„ ìµíˆëŠ” ê²ƒì´ë‹¤.

---

## 1. ëª¨ë¸ì„ ë¹„êµí•´ì•¼ í•˜ëŠ” ì´ìœ 

ëª¨ë¸ì˜ ì„±ëŠ¥ì€ **í›ˆë ¨ ë°ì´í„°ê°€ ì•„ë‹ˆë¼ ìƒˆë¡œìš´ ë°ì´í„°**ì—ì„œ ì–¼ë§ˆë‚˜ ì˜ ë§ì¶”ëŠ”ì§€ê°€ ì¤‘ìš”í•˜ë‹¤.  
ì´ë¥¼ ìœ„í•´ ìš°ë¦¬ëŠ” **Validation ë°ì´í„°**ì™€ **í‰ê°€ì§€í‘œ(MAE)** ë¥¼ ì‚¬ìš©í•œë‹¤.

ì´ì œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ë°”ê¿”ê°€ë©° ì–´ë–¤ ëª¨ë¸ì´ ê°€ì¥ ì¢‹ì€ ì˜ˆì¸¡ì„ í•˜ëŠ”ì§€ ì‹¤í—˜í•  ìˆ˜ ìˆë‹¤.

---

## 2. Decision Treeì™€ ê¹Šì´(Depth)

Decision Tree ëª¨ë¸ì—ëŠ” ë‹¤ì–‘í•œ ì˜µì…˜ì´ ìˆìœ¼ë©°, ê·¸ì¤‘ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†ŒëŠ” **íŠ¸ë¦¬ì˜ ê¹Šì´(depth)** ì´ë‹¤.

- íŠ¸ë¦¬ì˜ ê¹Šì´ = ì˜ˆì¸¡ ì „ì— ëª‡ ë²ˆì˜ ë¶„ê¸°(split)ë¥¼ ê±°ì¹˜ëŠ”ì§€
- ê¹Šì„ìˆ˜ë¡ ë” ë§ì€ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìª¼ê° ë‹¤

### ì˜ˆì‹œ
- ê¹Šì´ 1 â†’ 2ê°œì˜ ê·¸ë£¹
- ê¹Šì´ 2 â†’ 4ê°œì˜ ê·¸ë£¹
- ê¹Šì´ 10 â†’ 2Â¹â° = **1024ê°œì˜ leaf ë…¸ë“œ**

íŠ¸ë¦¬ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ê° leafì— í¬í•¨ëœ ë°ì´í„° ìˆ˜ëŠ” ì¤„ì–´ë“ ë‹¤.

---

## 3. Overfitting (ê³¼ì í•©)

íŠ¸ë¦¬ê°€ ë„ˆë¬´ ê¹Šìœ¼ë©´:

- ê° leafì— ë°ì´í„°ê°€ ê±°ì˜ ì—†ìŒ
- í›ˆë ¨ ë°ì´í„°ì—ëŠ” ê±°ì˜ ì™„ë²½í•˜ê²Œ ë§ìŒ
- ìƒˆë¡œìš´ ë°ì´í„°(Validation/Test)ì—ëŠ” ì„±ëŠ¥ ê¸‰ë½

ğŸ“Œ **ê³¼ì í•©**ì´ë€  
> í›ˆë ¨ ë°ì´í„°ì˜ ì¡ìŒ(noise)ê¹Œì§€ ì™¸ì›Œë²„ë¦° ìƒíƒœ

---

## 4. Underfitting (ê³¼ì†Œì í•©)

íŠ¸ë¦¬ê°€ ë„ˆë¬´ ì–•ìœ¼ë©´:

- ë°ì´í„°ê°€ ì¶©ë¶„íˆ êµ¬ë¶„ë˜ì§€ ì•ŠìŒ
- í›ˆë ¨ ë°ì´í„°ì—ì„œë„ ì„±ëŠ¥ì´ ë‚˜ì¨
- ì¤‘ìš”í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ì§€ ëª»í•¨

ğŸ“Œ **ê³¼ì†Œì í•©**ì´ë€  
> ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ í•µì‹¬ íŒ¨í„´ì„ ì¡ì§€ ëª»í•œ ìƒíƒœ

---

## 5. ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒ: Sweet Spot

ëª©í‘œëŠ” **ê³¼ì†Œì í•©ê³¼ ê³¼ì í•© ì‚¬ì´ì˜ ê· í˜•ì **ì„ ì°¾ëŠ” ê²ƒì´ë‹¤.

- í›ˆë ¨ ì„±ëŠ¥ âŒ
- ê²€ì¦ ì„±ëŠ¥ â­• ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒ

ì¦‰, **Validation MAEê°€ ê°€ì¥ ë‚®ì€ ëª¨ë¸**ì´ ìµœì ì˜ ëª¨ë¸ì´ë‹¤.

---

## 6. max_leaf_nodesë¡œ ëª¨ë¸ ë³µì¡ë„ ì¡°ì ˆ

`max_leaf_nodes`ëŠ” íŠ¸ë¦¬ê°€ ë§Œë“¤ ìˆ˜ ìˆëŠ” **ìµœëŒ€ leaf ê°œìˆ˜**ë¥¼ ì œí•œí•œë‹¤.

- ê°’ì´ ì‘ìŒ â†’ ë‹¨ìˆœí•œ ëª¨ë¸ â†’ ê³¼ì†Œì í•© ìœ„í—˜
- ê°’ì´ í¼ â†’ ë³µì¡í•œ ëª¨ë¸ â†’ ê³¼ì í•© ìœ„í—˜

---

## 7. MAEë¥¼ ë¹„êµí•˜ëŠ” í•¨ìˆ˜

```python
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor

def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
    model.fit(train_X, train_y)
    preds_val = model.predict(val_X)
    mae = mean_absolute_error(val_y, preds_val)
    return mae
```

ì´ í•¨ìˆ˜ëŠ”:
1. ì£¼ì–´ì§„ `max_leaf_nodes`ë¡œ ëª¨ë¸ ìƒì„±
2. í›ˆë ¨ ë°ì´í„°ë¡œ í•™ìŠµ
3. ê²€ì¦ ë°ì´í„°ë¡œ ì˜ˆì¸¡
4. MAE ë°˜í™˜

---

## 8. ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ

```python
for max_leaf_nodes in [5, 50, 500, 5000]:
    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)
    print(
        "Max leaf nodes: %d \t\t Mean Absolute Error: %d"
        % (max_leaf_nodes, my_mae)
    )
```

### ê²°ê³¼ í•´ì„

| max_leaf_nodes | MAE | í•´ì„ |
|---------------|-----|------|
| 5 | ë§¤ìš° í¼ | ê³¼ì†Œì í•© |
| 50 | ê°ì†Œ | ê°œì„ ë¨ |
| **500** | **ìµœì†Œ** | âœ… ìµœì  |
| 5000 | ë‹¤ì‹œ ì¦ê°€ | ê³¼ì í•© ì‹œì‘ |

---

## 9. ê²°ë¡  ì •ë¦¬

ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ì˜ ì›ì¸ì€ ë‘ ê°€ì§€ë‹¤.

### ğŸ”´ Overfitting
- í›ˆë ¨ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ë§ì¶¤
- ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ì €í•˜

### ğŸ”µ Underfitting
- ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœ
- í›ˆë ¨ ë°ì´í„°ì—ì„œë„ ì„±ëŠ¥ ì €í•˜

### âœ… í•´ê²° ë°©ë²•
- Validation ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ì„±ëŠ¥ í‰ê°€
- ì—¬ëŸ¬ ëª¨ë¸ì„ ë¹„êµ
- **Validation MAEê°€ ê°€ì¥ ë‚®ì€ ëª¨ë¸ ì„ íƒ**
